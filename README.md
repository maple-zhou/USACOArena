# CompeteMAS (Competition Multi-Agent System)

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![uv](https://img.shields.io/badge/package%20manager-uv-orange.svg)](https://docs.astral.sh/uv/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

This repository contains the supplementary code for NeurIPS 2025 paper under review: **"CompeteMAS: Cost-Aware Evaluation of Agentic Coding Capabilities of Multi-Agent Systems"**, and is for review only.

CompeteMAS is a comprehensive Online Judge (OJ) system designed to evaluate the coding capabilities of Multi-Agent Systems (MAS) in competitive programming environments. It features cost-aware evaluation, real-time competition management, and integration with modern LLM APIs.

## ğŸš€ Features

- **ğŸ† Multi-Agent Competition**: Support for multiple LLM agents competing simultaneously
- **ğŸ’° Cost-Aware Evaluation**: Token-based resource management and cost tracking
- **âš¡ Real-time API**: RESTful API for competition management and monitoring
- **ğŸ” Intelligent Hints**: Multi-level hint system with semantic and episodic knowledge
- **ğŸ“Š Comprehensive Analytics**: Detailed scoring, rankings, and performance metrics
- **ğŸ›¡ï¸ Secure Execution**: Sandboxed code execution via Rust-based judge
- **ğŸ—ï¸ Modular Architecture**: Clean separation of core framework and user customizations
- **ğŸ“ˆ High Performance**: Optimized storage system with 99.8% space savings

## ğŸ“‹ Prerequisites

- **Python 3.10+**
- **uv** (recommended package manager)
- **Rust & Cargo** (for online judge)
- **Docker** (for containerized deployment)

## ğŸ› ï¸ Installation

### 1. Clone the Repository
   ```bash
git clone <repository-url>
cd CompeteMAS
   ```

### 2. Install with uv (Recommended)
   ```bash
# Install uv if not already installed
curl -LsSf https://astral.sh/uv/install.sh | sh

# Create virtual environment and install dependencies
uv sync

# Activate virtual environment
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   ```

### 3. Prepare USACO Dataset

   Download the USACO data from the [link](https://drive.google.com/file/d/1z5ODOJMqyer1QxzYtEUZ2hbAx-7nU8Vi/view?usp=share_link) provided by [USACO Bench](https://github.com/princeton-nlp/USACO).

```bash
# Extract and place in data directory
unzip usaco_data.zip
mv data_copy dataset/datasets/usaco_2025
```

## ğŸ”§ Online Judge Setup

**Note**: The online judge system is based on the [online-judge-rust](https://github.com/cpinitiative/online-judge-rust) project. This is a third-party codebase and is not included in this repository.

### 1. Get Online Judge Rust
   ```bash
# Clone the online judge repository
   git clone https://github.com/cpinitiative/online-judge-rust.git
   ```

### 2. Install Rust Dependencies
   ```bash
# Install cargo-lambda
cargo install cargo-lambda
cargo lambda --help  # Verify installation

# Install zig (for cross-compilation)
sudo snap install zig --classic --beta
zig version  # Verify installation
   ```

### 3. Build and Run Online Judge
   ```bash
# Build the Lambda function
   cargo lambda build

# Build Docker image
   docker build --platform linux/amd64 -t oj-rust .

# Run the online judge
   docker run --platform linux/amd64 -p 9000:8080 oj-rust
   ```

### 4. Test Online Judge
   ```bash
   curl -X POST "http://localhost:9000/2015-03-31/functions/function/invocations" \
   -d '{
      "version": "2.0",
      "rawPath": "/compile-and-execute",
      "requestContext": {
         "http": {
         "method": "POST",
         "path": "/compile-and-execute"
         }
      },
      "headers": {
         "Content-Type": "application/json"
      },
      "body": "{\"compile\":{\"source_code\":\"#include <iostream>\\nusing namespace std;\\n\\nint main() {\\n  int a, b;\\n  cin >> a >> b;\\n  cout << a + b << endl;\\n  return 0;\\n}\",\"compiler_options\":\"-O2 -std=c++17\",\"language\":\"cpp\"},\"execute\":{\"stdin\":\"5 7\",\"timeout_ms\":5000}}",
      "isBase64Encoded": false
   }'
   ```

**Important**: Make sure the online judge is running on port 9000 before starting CompeteMAS competitions.

## ğŸ—ï¸ Architecture

CompeteMAS v0.2.0 é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œå®ç°äº†**æ ¸å¿ƒæ¡†æ¶**ä¸**ç”¨æˆ·è‡ªå®šä¹‰å†…å®¹**çš„æ¸…æ™°åˆ†ç¦»ï¼š

```
CompeteMAS/
â”œâ”€â”€ ğŸ—ï¸ æ ¸å¿ƒæ¡†æ¶åŒ…
â”‚   â”œâ”€â”€ core/                     # æ ¸å¿ƒä¸šåŠ¡é€»è¾‘
â”‚   â”‚   â”œâ”€â”€ models.py            # æ•°æ®æ¨¡å‹å®šä¹‰
â”‚   â”‚   â”œâ”€â”€ storage.py           # DuckDBå­˜å‚¨ç³»ç»Ÿ
â”‚   â”‚   â”œâ”€â”€ judge.py             # ä»£ç è¯„åˆ¤ç³»ç»Ÿ
â”‚   â”‚   â”œâ”€â”€ competition.py       # ç«èµ›æ ¸å¿ƒé€»è¾‘
â”‚   â”‚   â””â”€â”€ agent_interface.py   # æ™ºèƒ½ä½“æ¥å£æŠ½è±¡
â”‚   â”œâ”€â”€ REST APIæœåŠ¡
â”‚   â”‚   â””â”€â”€ server.py            # Flask APIæœåŠ¡å™¨
â”‚   â”œâ”€â”€ utils/                   # å·¥å…·æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ problem_loader.py    # USACOé—®é¢˜åŠ è½½å™¨
â”‚   â”‚   â””â”€â”€ conversation_logger.py # å¯¹è¯æ—¥å¿—è®°å½•
â”‚   â””â”€â”€ main.py                  # æ¡†æ¶ä¸»å…¥å£
â”œâ”€â”€ ğŸ› ï¸ ç”¨æˆ·è‡ªå®šä¹‰è„šæœ¬
â”‚   â”œâ”€â”€ agents/                  # è‡ªå®šä¹‰æ™ºèƒ½ä½“å®ç°
â”‚   â”‚   â””â”€â”€ single_agent.py     # LLMæ™ºèƒ½ä½“ç±»
â”‚   â”œâ”€â”€ prompts/                 # è‡ªå®šä¹‰æç¤ºè¯æ¨¡æ¿
â”‚   â”‚   â””â”€â”€ prompt_manager.py    # æç¤ºè¯ç³»ç»Ÿ
â”‚   â””â”€â”€ run_competition.py       # ç«èµ›è¿è¡Œä¸»è„šæœ¬
â”œâ”€â”€ ğŸ“‹ ç¤ºä¾‹å’Œé…ç½®æ¨¡æ¿
â”‚   â””â”€â”€ sample_configs/          # ç¤ºä¾‹é…ç½®æ–‡ä»¶
â”œâ”€â”€ é…ç½®æ–‡ä»¶ç›®å½•
â”œâ”€â”€ ğŸ“Š æ•°æ®å­˜å‚¨ç›®å½•
â””â”€â”€ logs/                        # æ—¥å¿—ç›®å½•
```

### æ¨¡å—åŒ–è®¾è®¡ä¼˜åŠ¿

#### 1. æ¸…æ™°çš„èŒè´£åˆ†ç¦»
- **æ ¸å¿ƒæ¡†æ¶** (`competemas/`) - ç¨³å®šçš„ä¸šåŠ¡é€»è¾‘å’ŒåŸºç¡€è®¾æ–½
- **ç”¨æˆ·è„šæœ¬** (`scripts/`) - å¯è‡ªå®šä¹‰çš„æ™ºèƒ½ä½“ã€æç¤ºè¯å’Œè¿è¡Œè„šæœ¬
- **ç¤ºä¾‹é…ç½®** (`examples/`) - é…ç½®æ¨¡æ¿å’Œæ–‡æ¡£

#### 2. æ™ºèƒ½ä½“æ¥å£è®¾è®¡
åˆ›å»ºäº†`AgentInterface`æŠ½è±¡æ¥å£ï¼Œå®ç°æ¾è€¦åˆï¼š

```python
# competemas/core/agent_interface.py
class AgentInterface(ABC):
    @abstractmethod
    async def process(self, state: Dict) -> Dict:
        """å¤„ç†ç«èµ›çŠ¶æ€ï¼Œç”Ÿæˆä¸‹ä¸€æ­¥è¡ŒåŠ¨"""
        pass
```

#### 3. æ€§èƒ½ä¼˜åŒ–
- **å­˜å‚¨ä¼˜åŒ–**ï¼šDuckDBæ•°æ®åº“å¤§å°ä»972MBé™è‡³2.3MB (99.8%èŠ‚çœ)
- **åŠ¨æ€åŠ è½½**ï¼šæµ‹è¯•ç”¨ä¾‹æŒ‰éœ€ä»æ–‡ä»¶ç³»ç»ŸåŠ è½½ï¼Œé¦–æ¬¡è®¿é—®ä»…+10-50ms
- **æ¨¡å—åŒ–æ¶æ„**ï¼šæ”¯æŒå¹¶è¡Œå¼€å‘ï¼Œæ˜“äºç»´æŠ¤å’Œæ‰©å±•

## ğŸ¯ Usage

### Quick Start

#### 1. å¯åŠ¨APIæœåŠ¡å™¨
   ```bash
# ä½¿ç”¨æ–°çš„æ¡†æ¶å…¥å£
python -m competemas.main --host 0.0.0.0 --port 5000

# æˆ–è€…ç›´æ¥è¿è¡Œ
cd competemas
python main.py --debug
   ```

#### 2. é…ç½®å‚èµ›è€…
ç¼–è¾‘ `examples/sample_configs/competitors_config.json`:
```json
{
  "competitors": [
    {
      "name": "gpt-4",
      "type": "generic",
      "model_id": "gpt-4",
      "api_base_url": "https://api.openai.com/v1",
      "api_key": "your-api-key"
    }
  ]
}
```

#### 3. è¿è¡Œç«èµ›
   ```bash
# ä½¿ç”¨ç”¨æˆ·è‡ªå®šä¹‰è„šæœ¬
python scripts/run_competition.py \
    --competition-config examples/sample_configs/competition_config.json \
    --competitors-config examples/sample_configs/competitors_config.json \
    --problem-ids examples/sample_configs/problem_ids.json
```

### è‡ªå®šä¹‰æ™ºèƒ½ä½“å¼€å‘

åœ¨`agents/single_agent/single_agent.py`ä¸­å®ç°æ‚¨çš„æ™ºèƒ½ä½“ï¼š

```python
from competemas.core.agent_interface import AgentInterface

class MyCustomAgent(AgentInterface):
    async def process(self, state: Dict) -> Dict:
        # å®ç°æ‚¨çš„æ™ºèƒ½ä½“é€»è¾‘
        return {"action": "VIEW_PROBLEMS"}
```

### API Usage

ç³»ç»Ÿæä¾›å…¨é¢çš„REST APIï¼š

```bash
# åˆ—å‡ºæ‰€æœ‰ç«èµ›
curl http://localhost:5000/api/competitions

# åˆ›å»ºç«èµ›
curl -X POST http://localhost:5000/api/competitions \
  -H "Content-Type: application/json" \
  -d '{
    "title": "Test Competition",
    "description": "A test competition",
    "problem_ids": ["1323_bronze_feb"],
    "max_tokens_per_participant": 100000
  }'

# è·å–ç«èµ›è¯¦æƒ…
curl http://localhost:5000/api/competitions/{competition_id}

# æŸ¥çœ‹æ’å
curl http://localhost:5000/api/competitions/{competition_id}/rankings
```

## ğŸ”§ Development

### Setup Development Environment
```bash
# Install development dependencies
uv sync --extra dev

# Run tests
uv run pytest

# Format code
uv run black competemas/ scripts/ tests/

# Lint code
uv run ruff check competemas/ scripts/ tests/

# Type checking
uv run mypy competemas/
```

### é¡¹ç›®ç»“æ„è¯¦è§£

#### æ ¸å¿ƒæ¡†æ¶ (`competemas/`)
- **`core/`**: æ ¸å¿ƒä¸šåŠ¡é€»è¾‘
  - `models.py`: æ•°æ®æ¨¡å‹å’Œç±»å‹å®šä¹‰
  - `storage.py`: DuckDBå­˜å‚¨ç³»ç»Ÿï¼Œæ”¯æŒé«˜æ€§èƒ½æŸ¥è¯¢
  - `judge.py`: ä»£ç è¯„åˆ¤å’Œæ‰§è¡Œç³»ç»Ÿ
  - `competition.py`: ç«èµ›ç”Ÿå‘½å‘¨æœŸç®¡ç†
  - `agent_interface.py`: æ™ºèƒ½ä½“æŠ½è±¡æ¥å£

- **`api/`**: REST APIæ¥å£
  - `server.py`: Flask APIæœåŠ¡å™¨ï¼Œæä¾›å®Œæ•´çš„RESTfulæ¥å£

- **`utils/`**: å·¥å…·å‡½æ•°
  - `problem_loader.py`: USACOé—®é¢˜åŠ¨æ€åŠ è½½
  - `conversation_logger.py`: å¯¹è¯æ—¥å¿—è®°å½•

#### ç”¨æˆ·è‡ªå®šä¹‰ (`scripts/`)
- **`agents/`**: æ™ºèƒ½ä½“å®ç°
  - `single_agent.py`: æ”¯æŒå¤šç§LLMæä¾›å•†çš„é€šç”¨æ™ºèƒ½ä½“

- **`prompts/`**: æç¤ºè¯ç®¡ç†
  - `prompt_manager.py`: æç¤ºè¯æ¨¡æ¿å’Œè§£æç³»ç»Ÿ

- **`run_competition.py`**: ç«èµ›æ‰§è¡Œè„šæœ¬

#### é…ç½®å’Œç¤ºä¾‹ (`examples/`)
- **`sample_configs/`**: é…ç½®æ–‡ä»¶æ¨¡æ¿
  - ç«èµ›é…ç½®ã€å‚èµ›è€…é…ç½®ã€é—®é¢˜åˆ—è¡¨ç­‰

## ğŸ“Š Competition System

### Agent Response Format
ç«èµ›ç³»ç»Ÿå‘æ™ºèƒ½ä½“è¿”å›ç»“æ„åŒ–æ•°æ®ï¼š

  ```python
  {
  "competition_id": str,           # å½“å‰ç«èµ›ID
  "competition_details": {         # ç«èµ›è¯¦æƒ…
          "id": str,
          "title": str,
          "description": str,
          "problem_ids": List[str],
          "rules": Dict
      },
  "competitor_state": {            # å½“å‰å‚èµ›è€…çŠ¶æ€
      "name": str,                 # å‚èµ›è€…åç§°
      "remaining_tokens": int,     # å‰©ä½™ä»¤ç‰Œæ•°
        "solved_problems": List[str], # å·²è§£å†³é—®é¢˜åˆ—è¡¨
      "is_running": bool,          # æ˜¯å¦ä»åœ¨è¿è¡Œ
        "termination_reason": Optional[str], # ç»ˆæ­¢åŸå› ï¼ˆå¦‚æœæœ‰ï¼‰
      "score": int,                # å½“å‰å¾—åˆ†
      "score": int           # æœ€ç»ˆå¾—åˆ†
      },
  "problems": List[Dict],          # æ‰€æœ‰é—®é¢˜åˆ—è¡¨
  "rankings": List[Dict],          # å½“å‰æ’å
  "last_action_result": {          # ä¸Šæ¬¡æ“ä½œç»“æœ
      "status": str,               # "success" æˆ– "error"
      "data": Dict,                # æ“ä½œè¿”å›æ•°æ®
      "message": str               # é”™è¯¯æ¶ˆæ¯ï¼ˆå¦‚æœæœ‰ï¼‰
      },
  "other_competitors_status": [    # å…¶ä»–å‚èµ›è€…çŠ¶æ€
          {
              "name": str,
              "is_terminated": bool,
              "termination_reason": Optional[str]
          }
      ]
  }
  ```

### Available Actions
1. **VIEW_PROBLEM**: æŸ¥çœ‹é—®é¢˜è¯¦æƒ…
2. **GET_HINT**: è¯·æ±‚æç¤ºï¼ˆæ¶ˆè€—ä»¤ç‰Œï¼‰
3. **submission_SOLUTION**: æäº¤ä»£ç è§£å†³æ–¹æ¡ˆ
4. **TERMINATE**: ç»“æŸå‚ä¸

## ğŸ”„ è¿ç§»æŒ‡å—

å¦‚æœæ‚¨æœ‰åŸºäºæ—§ç»“æ„ï¼ˆsrc/ç›®å½•ï¼‰çš„ä»£ç ï¼Œè¯·æŒ‰ä»¥ä¸‹æ­¥éª¤è¿ç§»ï¼š

### 1. æ›´æ–°å¯¼å…¥è·¯å¾„
```python
# æ—§çš„å¯¼å…¥æ–¹å¼
from src.competemas.core.agents import GenericAPIAgent

# æ–°çš„å¯¼å…¥æ–¹å¼  
from agents import GenericAPIAgent
```

### 2. ç§»åŠ¨è‡ªå®šä¹‰ä»£ç 
- è‡ªå®šä¹‰æ™ºèƒ½ä½“ â†’ `agents/`
- è‡ªå®šä¹‰æç¤ºè¯ â†’ `scripts/prompts/`
- è¿è¡Œè„šæœ¬ â†’ `scripts/`

### 3. æ›´æ–°é…ç½®æ–‡ä»¶
- å¤åˆ¶é…ç½®æ¨¡æ¿ï¼š`examples/sample_configs/`
- æ ¹æ®éœ€è¦è°ƒæ•´é…ç½®å‚æ•°

## ğŸ”¬ For Reviewers

æˆ‘ä»¬çƒ­çƒˆæ¬¢è¿å®¡ç¨¿äººæ¢ç´¢å’Œè¯•éªŒæˆ‘ä»¬çš„ç³»ç»Ÿï¼

### Model Configuration
- åœ¨ `examples/sample_configs/competitors_config.json` ä¸­é…ç½®ä¸åŒçš„LLMæ¨¡å‹
- å…³é”®å‚æ•°: `model_id`, `api_base_url`, `api_key`
- å¯åœ¨ `agents/single_agent/single_agent.py` ä¸­è°ƒæ•´ä»¤ç‰Œå®šä»·
- å‚è€ƒ [Artificial Analysis](https://artificialanalysis.ai/) è·å–æ¨¡å‹å®šä»·ä¿¡æ¯

### Competition Parameters
- åœ¨ `examples/sample_configs/competition_config.json` ä¸­è°ƒæ•´ç«èµ›å‚æ•°
- ä¿®æ”¹ `examples/sample_configs/problem_ids.json` æµ‹è¯•ä¸åŒé—®é¢˜é›†
- æ‰€æœ‰å¯ç”¨é—®é¢˜åˆ—åœ¨ `config/all_problems.json` ä¸­

### Custom MAS Development
- åœ¨ `scripts/prompts/prompt_manager.py` ä¸­ä¿®æ”¹æç¤ºè¯
- åœ¨ `agents/single_agent/single_agent.py` ä¸­è°ƒæ•´æ™ºèƒ½ä½“è¡Œä¸º
- æ™ºèƒ½ä½“é€šè¿‡ `Agent.process` å‡½æ•°è¿æ¥
- æ¬¢è¿å°è¯•ä¸åŒçš„ç­–ç•¥å’Œæ–¹æ³•ï¼ğŸ˜Š

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests and linting
5. submission a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Thanks to all contributors
- Inspired by various programming competition platforms
- Built with modern Python best practices 
- USACO problem library from [USACO Bench](https://github.com/princeton-nlp/USACO)
- Online Judge implementation from [CP Initiative](https://github.com/cpinitiative/online-judge-rust)

---

**CompeteMAS v0.2.0** - æ›´æ¨¡å—åŒ–ã€æ›´é«˜æ•ˆã€æ›´æ˜“æ‰©å±•çš„å¤šæ™ºèƒ½ä½“ç«èµ›æ¡†æ¶ ğŸ‰
