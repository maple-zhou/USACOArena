# CompeteMAS (Competition Multi-Agent System)

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![uv](https://img.shields.io/badge/package%20manager-uv-orange.svg)](https://docs.astral.sh/uv/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

This repository contains the supplementary code for NeurIPS 2025 paper under review: **"CompeteMAS: Cost-Aware Evaluation of Agentic Coding Capabilities of Multi-Agent Systems"**, and is for review only.

CompeteMAS is a comprehensive Online Judge (OJ) system designed to evaluate the coding capabilities of Multi-Agent Systems (MAS) in competitive programming environments. It features cost-aware evaluation, real-time competition management, and integration with modern LLM APIs.

## üöÄ Features

- **üèÜ Multi-Agent Competition**: Support for multiple LLM agents competing simultaneously
- **üí∞ Cost-Aware Evaluation**: Token-based resource management and cost tracking
- **‚ö° Real-time API**: RESTful API for competition management and monitoring
- **üîç Intelligent Hints**: Multi-level hint system with semantic and episodic knowledge
- **üìä Comprehensive Analytics**: Detailed scoring, rankings, and performance metrics
- **üê≥ Container Ready**: Docker support for easy deployment
- **üõ°Ô∏è Secure Execution**: Sandboxed code execution via Rust-based judge

## üìã Prerequisites

- **Python 3.10+**
- **uv** (recommended package manager)
- **Rust & Cargo** (for online judge)
- **Docker** (for containerized deployment)

## üõ†Ô∏è Installation

### 1. Clone the Repository
   ```bash
git clone <repository-url>
cd CompeteMAS
   ```

### 2. Install with uv (Recommended)
   ```bash
# Install uv if not already installed
curl -LsSf https://astral.sh/uv/install.sh | sh

# Create virtual environment and install dependencies
uv sync

# Activate virtual environment
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   ```

### 3. Prepare USACO Dataset

   Download the USACO data from the [link](https://drive.google.com/file/d/1z5ODOJMqyer1QxzYtEUZ2hbAx-7nU8Vi/view?usp=share_link) provided by [USACO Bench](https://github.com/princeton-nlp/USACO).

```bash
# Extract and place in data directory
unzip usaco_data.zip
mv data_copy data/datasets/usaco_2025
```

## üîß Online Judge Setup

**Note**: The online judge system is based on the [online-judge-rust](https://github.com/cpinitiative/online-judge-rust) project. This is a third-party codebase and is not included in this repository.

### 1. Get Online Judge Rust
   ```bash
# Clone the online judge repository
   git clone https://github.com/cpinitiative/online-judge-rust.git
   ```

### 2. Install Rust Dependencies
   ```bash
# Install cargo-lambda
cargo install cargo-lambda
cargo lambda --help  # Verify installation

# Install zig (for cross-compilation)
sudo snap install zig --classic --beta
zig version  # Verify installation
   ```

### 3. Build and Run Online Judge
   ```bash
# Build the Lambda function
   cargo lambda build

# Build Docker image
   docker build --platform linux/amd64 -t oj-rust .

# Run the online judge
   docker run --platform linux/amd64 -p 9000:8080 oj-rust
   ```

### 4. Test Online Judge
   ```bash
   curl -X POST "http://localhost:9000/2015-03-31/functions/function/invocations" \
   -d '{
      "version": "2.0",
      "rawPath": "/compile-and-execute",
      "requestContext": {
         "http": {
         "method": "POST",
         "path": "/compile-and-execute"
         }
      },
      "headers": {
         "Content-Type": "application/json"
      },
      "body": "{\"compile\":{\"source_code\":\"#include <iostream>\\nusing namespace std;\\n\\nint main() {\\n  int a, b;\\n  cin >> a >> b;\\n  cout << a + b << endl;\\n  return 0;\\n}\",\"compiler_options\":\"-O2 -std=c++17\",\"language\":\"cpp\"},\"execute\":{\"stdin\":\"5 7\",\"timeout_ms\":5000}}",
      "isBase64Encoded": false
   }'
   ```

**Important**: Make sure the online judge is running on port 9000 before starting CompeteMAS competitions.

## üéØ Usage

### Quick Start

1. **Start the API Server**
   ```bash
   uv run competemas --host 0.0.0.0 --port 5000
   ```

2. **Configure Competitors**
   Edit `config/competitors_config.json`:
   ```json
   {
     "competitors": [
       {
         "name": "gpt-4",
         "model_id": "gpt-4",
         "api_base": "https://api.openai.com/v1",
         "api_key": "your-api-key",
         "max_tokens": 100000
       }
     ]
   }
   ```

3. **Run Competition**
   ```bash
   uv run competemas_run
   ```

### API Usage

The system provides a comprehensive REST API:

```bash
# List competitions
curl http://localhost:5000/api/competitions

# Create competition
curl -X POST http://localhost:5000/api/competitions \
  -H "Content-Type: application/json" \
  -d '{
    "title": "Test Competition",
    "description": "A test competition",
    "problem_ids": ["1323_bronze_feb"],
    "max_tokens_per_participant": 100000
  }'

# Get competition details
curl http://localhost:5000/api/competitions/{competition_id}

# View rankings
curl http://localhost:5000/api/competitions/{competition_id}/rankings
```

## üèóÔ∏è Architecture

```
CompeteMAS/
‚îú‚îÄ‚îÄ üìÅ src/                          # Ê∫ê‰ª£Á†ÅÁõÆÂΩï
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ competemas/               # ‰∏ªÂåÖ
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ __init__.py           # ÂåÖÂàùÂßãÂåñ
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ main.py               # ‰∏ªÁ®ãÂ∫èÂÖ•Âè£
‚îÇ       ‚îú‚îÄ‚îÄ üìÅ api/                  # APIÊúçÂä°Ê®°Âùó
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py       # APIÊ®°ÂùóÂàùÂßãÂåñ
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ server.py         # Flask APIÊúçÂä°Âô®
‚îÇ       ‚îú‚îÄ‚îÄ üìÅ cli/                  # ÂëΩ‰ª§Ë°åÂ∑•ÂÖ∑Ê®°Âùó
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py       # CLIÊ®°ÂùóÂàùÂßãÂåñ
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ run_competition.py # Á´ûËµõËøêË°åÂ∑•ÂÖ∑
‚îÇ       ‚îú‚îÄ‚îÄ üìÅ core/                 # Ê†∏ÂøÉ‰∏öÂä°ÈÄªËæë
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py       # Ê†∏ÂøÉÊ®°ÂùóÂàùÂßãÂåñ
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ agents.py         # Êô∫ËÉΩ‰ΩìÂÆûÁé∞
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ competition.py    # Á´ûËµõÁÆ°ÁêÜÈÄªËæë
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ judge.py          # ËØÑÊµãÁ≥ªÁªü
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ models.py         # Êï∞ÊçÆÊ®°Âûã
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ storage.py        # Êï∞ÊçÆÂ≠òÂÇ®
‚îÇ       ‚îî‚îÄ‚îÄ üìÅ utils/                # Â∑•ÂÖ∑ÂáΩÊï∞Ê®°Âùó
‚îÇ           ‚îú‚îÄ‚îÄ üìÑ __init__.py       # Â∑•ÂÖ∑Ê®°ÂùóÂàùÂßãÂåñ
‚îÇ           ‚îú‚îÄ‚îÄ üìÑ conversation_logger.py # ÂØπËØùÊó•ÂøóÂ∑•ÂÖ∑
‚îÇ           ‚îú‚îÄ‚îÄ üìÑ problem_loader.py # ÈóÆÈ¢òÂä†ËΩΩÂô®
‚îÇ           ‚îî‚îÄ‚îÄ üìÑ prompts.py        # ÊèêÁ§∫ËØçÁÆ°ÁêÜ
‚îú‚îÄ‚îÄ üìÅ config/                       # ÈÖçÁΩÆÊñá‰ª∂ÁõÆÂΩï
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ all_problems.json        # ÊâÄÊúâÈóÆÈ¢òÈÖçÁΩÆ
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ competition_config.json  # Á´ûËµõÈÖçÁΩÆ
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ competitors_config.json  # ÂèÇËµõËÄÖÈÖçÁΩÆ
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ problem_ids.json         # ÈóÆÈ¢òIDÂàóË°®
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ prompts.json             # ÊèêÁ§∫ËØçÈÖçÁΩÆ
‚îú‚îÄ‚îÄ üìÅ data/                         # Êï∞ÊçÆÁõÆÂΩï
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ competitions/            # Á´ûËµõÊï∞ÊçÆ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ corpuses/                # ËØ≠ÊñôÂ∫ìÊï∞ÊçÆ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ datasets/                # Êï∞ÊçÆÈõÜ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ datasets_original/       # ÂéüÂßãÊï∞ÊçÆÈõÜ
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ submissions/             # Êèê‰∫§ËÆ∞ÂΩï
‚îú‚îÄ‚îÄ üìÅ logs/                         # Êó•ÂøóÁõÆÂΩï
‚îú‚îÄ‚îÄ üìÅ tests/                        # ÊµãËØï‰ª£Á†ÅÁõÆÂΩï
‚îú‚îÄ‚îÄ üìÑ pyproject.toml               # uvÈ°πÁõÆÈÖçÁΩÆ
‚îî‚îÄ‚îÄ üìÑ README.md                    # È°πÁõÆËØ¥ÊòéÊñáÊ°£
```

**Note**: The online judge system (`online-judge-rust`) is a separate third-party dependency that needs to be cloned and set up separately. See the [Online Judge Setup](#-online-judge-setup) section for details.

## üîß Development

### Setup Development Environment
```bash
# Install development dependencies
uv sync --extra dev

# Run tests
uv run pytest

# Format code
uv run black src/ tests/

# Lint code
uv run ruff check src/ tests/

# Type checking
uv run mypy src/
```

### Project Structure

- **`src/competemas/core/`**: Core business logic
  - `competition.py`: Competition lifecycle management
  - `agents.py`: Multi-agent framework implementation
  - `judge.py`: Code evaluation and scoring
  - `models.py`: Data models and schemas
  - `storage.py`: Data persistence layer

- **`src/competemas/api/`**: REST API interface
  - `server.py`: Flask API server with comprehensive endpoints

- **`src/competemas/cli/`**: Command-line tools
  - `run_competition.py`: Competition execution tool

- **`src/competemas/utils/`**: Utility functions
  - `problem_loader.py`: USACO problem loading
  - `prompts.py`: LLM prompt management
  - `conversation_logger.py`: Logging utilities

## üìä Competition System

### Agent Response Format
The competition system returns structured data to agents:

  ```python
  {
    "competition_id": str,           # Current competition ID
    "competition_details": {         # Competition details
          "id": str,
          "title": str,
          "description": str,
          "problem_ids": List[str],
          "rules": Dict
      },
    "competitor_state": {            # Current competitor state
        "name": str,                 # Competitor name
        "remaining_tokens": int,     # Remaining tokens
          "solved_problems": List[str], # List of solved problems
        "is_running": bool,          # Whether still running
          "termination_reason": Optional[str], # Termination reason if any
        "score": int,                # Current score
        "final_score": int           # Final score
      },
    "problems": List[Dict],          # List of all problems
    "rankings": List[Dict],          # Current rankings
    "last_action_result": {          # Result of the last action
        "status": str,               # "success" or "error"
        "data": Dict,                # Action return data
        "message": str               # Error message if any
      },
    "other_competitors_status": [    # Status of other competitors
          {
              "name": str,
              "is_terminated": bool,
              "termination_reason": Optional[str]
          }
      ]
  }
  ```

### Available Actions
1. **VIEW_PROBLEM**: View problem details
2. **GET_HINT**: Request hints (consumes tokens)
3. **SUBMIT_SOLUTION**: Submit code solution
4. **TERMINATE**: End participation

## üî¨ For Reviewers

We warmly welcome reviewers to explore and experiment with our system!

### Model Configuration
- Configure different LLM models in `config/competitors_config.json`
- Key parameters: `model_id`, `api_base`, `api_key`
- Token pricing can be adjusted in `competition.py` line 73
- Reference [Artificial Analysis](https://artificialanalysis.ai/) for model pricing

### Competition Parameters
- Adjust competition parameters in `config/competition_config.json`
- Modify `config/problem_ids.json` to test different problem sets
- All available problems are listed in `config/all_problems.json`

### Custom MAS Development
- Modify prompts in `prompts.py` and agent behaviors in `agents.py`
- Agents connect through `Agent.process` function
- Experiment with different strategies and approaches! üòä

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests and linting
5. Submit a pull request

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- Thanks to all contributors
- Inspired by various programming competition platforms
- Built with modern Python best practices 
- USACO problem library from [USACO Bench](https://github.com/princeton-nlp/USACO)
- Online Judge implementation from [CP Initiative](https://github.com/cpinitiative/online-judge-rust)
